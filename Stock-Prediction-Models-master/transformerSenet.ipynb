{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import mxnet\n",
    "# from mxnet import npx, nd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras import Model\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "from tqdm import tqdm\n",
    "sns.set()\n",
    "tf.compat.v1.random.set_random_seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-03-23</td>\n",
       "      <td>311.250000</td>\n",
       "      <td>311.250000</td>\n",
       "      <td>300.450012</td>\n",
       "      <td>301.540009</td>\n",
       "      <td>301.540009</td>\n",
       "      <td>6654900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-03-26</td>\n",
       "      <td>307.339996</td>\n",
       "      <td>307.589996</td>\n",
       "      <td>291.359985</td>\n",
       "      <td>304.179993</td>\n",
       "      <td>304.179993</td>\n",
       "      <td>8375200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-03-27</td>\n",
       "      <td>304.000000</td>\n",
       "      <td>304.269989</td>\n",
       "      <td>277.179993</td>\n",
       "      <td>279.179993</td>\n",
       "      <td>279.179993</td>\n",
       "      <td>13872000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-03-28</td>\n",
       "      <td>264.579987</td>\n",
       "      <td>268.679993</td>\n",
       "      <td>252.100006</td>\n",
       "      <td>257.779999</td>\n",
       "      <td>257.779999</td>\n",
       "      <td>21001400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-03-29</td>\n",
       "      <td>256.489990</td>\n",
       "      <td>270.959991</td>\n",
       "      <td>248.210007</td>\n",
       "      <td>266.130005</td>\n",
       "      <td>266.130005</td>\n",
       "      <td>15170700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date        Open        High         Low       Close   Adj Close  \\\n",
       "0  2018-03-23  311.250000  311.250000  300.450012  301.540009  301.540009   \n",
       "1  2018-03-26  307.339996  307.589996  291.359985  304.179993  304.179993   \n",
       "2  2018-03-27  304.000000  304.269989  277.179993  279.179993  279.179993   \n",
       "3  2018-03-28  264.579987  268.679993  252.100006  257.779999  257.779999   \n",
       "4  2018-03-29  256.489990  270.959991  248.210007  266.130005  266.130005   \n",
       "\n",
       "     Volume  \n",
       "0   6654900  \n",
       "1   8375200  \n",
       "2  13872000  \n",
       "3  21001400  \n",
       "4  15170700  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./dataset/TSLA.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.395163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.415627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.221843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.055965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.120688</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0\n",
       "0  0.395163\n",
       "1  0.415627\n",
       "2  0.221843\n",
       "3  0.055965\n",
       "4  0.120688"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minmax = MinMaxScaler().fit(df.iloc[:, 4:5].astype('float32')) # Close index\n",
    "df_log = minmax.transform(df.iloc[:, 4:5].astype('float32')) # Close index\n",
    "df_log = pd.DataFrame(df_log)\n",
    "df_log.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((251, 7), (221, 1), (30, 1))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_size = 30\n",
    "simulation_size = 10\n",
    "\n",
    "df_train = df_log.iloc[:-test_size]\n",
    "df_test = df_log.iloc[-test_size:]\n",
    "df.shape, df_train.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 30\n",
    "class Time2Vector(Layer):\n",
    "  def __init__(self, seq_len, **kwargs):\n",
    "    super(Time2Vector, self).__init__()\n",
    "    self.seq_len = seq_len\n",
    "\n",
    "  def build(self, input_shape):\n",
    "    self.weights_linear = self.add_weight(name='weight_linear',\n",
    "                                shape=(int(self.seq_len),),\n",
    "                                initializer='uniform',\n",
    "                                trainable=True)\n",
    "    \n",
    "    self.bias_linear = self.add_weight(name='bias_linear',\n",
    "                                shape=(int(self.seq_len),),\n",
    "                                initializer='uniform',\n",
    "                                trainable=True)\n",
    "    \n",
    "    self.weights_periodic = self.add_weight(name='weight_periodic',\n",
    "                                shape=(int(self.seq_len),),\n",
    "                                initializer='uniform',\n",
    "                                trainable=True)\n",
    "\n",
    "    self.bias_periodic = self.add_weight(name='bias_periodic',\n",
    "                                shape=(int(self.seq_len),),\n",
    "                                initializer='uniform',\n",
    "                                trainable=True)\n",
    "\n",
    "  def call(self, x):\n",
    "    x = tf.math.reduce_mean(x[:,:,:4], axis=-1) # Convert (batch, seq_len, 5) to (batch, seq_len)\n",
    "    time_linear = self.weights_linear * x + self.bias_linear\n",
    "    time_linear = tf.expand_dims(time_linear, axis=-1) # (batch, seq_len, 1)\n",
    "    \n",
    "    time_periodic = tf.math.sin(tf.multiply(x, self.weights_periodic) + self.bias_periodic)\n",
    "    time_periodic = tf.expand_dims(time_periodic, axis=-1) # (batch, seq_len, 1)\n",
    "    return tf.concat([time_linear, time_periodic], axis=-1) # (batch, seq_len, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleAttention(Layer):\n",
    "  def __init__(self, d_k, d_v):\n",
    "    super(SingleAttention, self).__init__()\n",
    "    self.d_k = d_k\n",
    "    self.d_v = d_v\n",
    "\n",
    "  def build(self, input_shape):\n",
    "    self.query = Dense(self.d_k, input_shape=input_shape, kernel_initializer='glorot_uniform', bias_initializer='glorot_uniform')\n",
    "    self.key = Dense(self.d_k, input_shape=input_shape, kernel_initializer='glorot_uniform', bias_initializer='glorot_uniform')\n",
    "    self.value = Dense(self.d_v, input_shape=input_shape, kernel_initializer='glorot_uniform', bias_initializer='glorot_uniform')\n",
    "\n",
    "  def call(self, inputs): # inputs = (in_seq, in_seq, in_seq)\n",
    "    q = self.query(inputs[0])\n",
    "    k = self.key(inputs[1])\n",
    "\n",
    "    attn_weights = tf.matmul(q, k, transpose_b=True)\n",
    "    attn_weights = tf.map_fn(lambda x: x/np.sqrt(self.d_k), attn_weights)\n",
    "    attn_weights = tf.nn.softmax(attn_weights, axis=-1)\n",
    "    \n",
    "    v = self.value(inputs[2])\n",
    "    attn_out = tf.matmul(attn_weights, v)\n",
    "    return attn_out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiAttention(Layer):\n",
    "  def __init__(self, d_k, d_v, n_heads):\n",
    "    super(MultiAttention, self).__init__()\n",
    "    self.d_k = d_k\n",
    "    self.d_v = d_v\n",
    "    self.n_heads = n_heads\n",
    "    self.attn_heads = list()\n",
    "\n",
    "  def build(self, input_shape):\n",
    "    for n in range(self.n_heads):\n",
    "      self.attn_heads.append(SingleAttention(self.d_k, self.d_v))  \n",
    "    self.linear = Dense(7, input_shape=input_shape, kernel_initializer='glorot_uniform', bias_initializer='glorot_uniform')\n",
    "\n",
    "  def call(self, inputs):\n",
    "    attn = [self.attn_heads[i](inputs) for i in range(self.n_heads)]\n",
    "    concat_attn = tf.concat(attn, axis=-1)\n",
    "    multi_linear = self.linear(concat_attn)\n",
    "    return multi_linear "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(Layer):\n",
    "  def __init__(self, d_k, d_v, n_heads, ff_dim, dropout=0.1, **kwargs):\n",
    "    super(TransformerEncoder, self).__init__()\n",
    "    self.d_k = d_k\n",
    "    self.d_v = d_v\n",
    "    self.n_heads = n_heads\n",
    "    self.ff_dim = ff_dim\n",
    "    self.attn_heads = list()\n",
    "    self.dropout_rate = dropout\n",
    "\n",
    "  def build(self, input_shape):\n",
    "    self.attn_multi = MultiAttention(self.d_k, self.d_v, self.n_heads)\n",
    "    self.attn_dropout = Dropout(self.dropout_rate)\n",
    "    self.attn_normalize = LayerNormalization(input_shape=input_shape, epsilon=1e-6)\n",
    "\n",
    "    self.ff_conv1D_1 = Conv1D(filters=self.ff_dim, kernel_size=1, activation='relu')\n",
    "    self.ff_conv1D_2 = Conv1D(filters=8, kernel_size=1) # input_shape[0]=(batch, seq_len, 7), input_shape[0][-1]=7 \n",
    "    self.ff_dropout = Dropout(self.dropout_rate)\n",
    "    self.ff_normalize = LayerNormalization(input_shape=input_shape, epsilon=1e-6)    \n",
    "  \n",
    "  def call(self, inputs): # inputs = (in_seq, in_seq, in_seq)\n",
    "    attn_layer = self.attn_multi(inputs)\n",
    "    attn_layer = self.attn_dropout(attn_layer)\n",
    "    attn_layer = self.attn_normalize(inputs[0] + attn_layer)\n",
    "\n",
    "    ff_layer = self.ff_conv1D_1(attn_layer)\n",
    "    ff_layer = self.ff_conv1D_2(ff_layer)\n",
    "    ff_layer = self.ff_dropout(ff_layer)\n",
    "    ff_layer = self.ff_normalize(inputs[0] + ff_layer)\n",
    "    return ff_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_k = 30\n",
    "d_v = 30\n",
    "ff_dim = 2\n",
    "n_heads = 3\n",
    "timestamp = test_size\n",
    "epoch = 300\n",
    "future_day = test_size\n",
    "learning_rate = 5e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(\n",
    "        self,\n",
    "        learning_rate,\n",
    "    ):\n",
    "        self.\n",
    "        self.X = tf.compat.v1.placeholder(tf.float32, (None, None, size))\n",
    "        self.Y = tf.compat.v1.placeholder(tf.float32, (None, output_size))\n",
    "\n",
    "        encoder_embedded = tf.compat.v1.layers.dense(self.X, size_layer)\n",
    "        encoder_embedded += position_encoding(encoder_embedded)\n",
    "        \n",
    "        e = tf.identity(encoder_embedded)\n",
    "        for i in range(num_layers): \n",
    "            dilation_rate = 2 ** i\n",
    "            pad_sz = (kernel_size - 1) * dilation_rate \n",
    "            with tf.compat.v1.variable_scope('block_%d'%i):\n",
    "                encoder_embedded += cnn_block(encoder_embedded, dilation_rate, \n",
    "                                              pad_sz, size_layer, kernel_size)\n",
    "                \n",
    "        encoder_output, output_memory = encoder_embedded, encoder_embedded + e\n",
    "        g = tf.identity(encoder_embedded)\n",
    "\n",
    "        for i in range(num_layers):\n",
    "            dilation_rate = 2 ** i\n",
    "            pad_sz = (kernel_size - 1) * dilation_rate\n",
    "            with tf.compat.v1.variable_scope('decode_%d'%i):\n",
    "                attn_res = h = cnn_block(encoder_embedded, dilation_rate, \n",
    "                                         pad_sz, size_layer, kernel_size)\n",
    "\n",
    "            C = []\n",
    "            for j in range(n_attn_heads):\n",
    "                h_ = tf.compat.v1.layers.dense(h, size_layer // n_attn_heads)\n",
    "                g_ = tf.compat.v1.layers.dense(g, size_layer // n_attn_heads)\n",
    "                zu_ = tf.compat.v1.layers.dense(\n",
    "                    encoder_output, size_layer // n_attn_heads\n",
    "                )\n",
    "                ze_ = tf.compat.v1.layers.dense(output_memory, size_layer // n_attn_heads)\n",
    "\n",
    "                d = tf.compat.v1.layers.dense(h_, size_layer // n_attn_heads) + g_\n",
    "                dz = tf.matmul(d, tf.transpose(zu_, [0, 2, 1]))\n",
    "                a = tf.nn.softmax(dz)\n",
    "                c_ = tf.matmul(a, ze_)\n",
    "                C.append(c_)\n",
    "\n",
    "            c = tf.concat(C, 2)\n",
    "            h = tf.compat.v1.layers.dense(attn_res + c, size_layer)\n",
    "            h = tf.nn.dropout(h, keep_prob = dropout)\n",
    "            encoder_embedded += h\n",
    "\n",
    "        encoder_embedded = tf.sigmoid(encoder_embedded[-1])\n",
    "        self.logits = tf.compat.v1.layers.dense(encoder_embedded, output_size)\n",
    "        self.cost = tf.reduce_mean(tf.square(self.Y - self.logits))\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate).minimize(\n",
    "            self.cost\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "  '''Initialize time and transformer layers'''\n",
    "  time_embedding = Time2Vector(seq_len)\n",
    "  attn_layer1 = TransformerEncoder(d_k, d_v, n_heads, ff_dim)\n",
    "  attn_layer2 = TransformerEncoder(d_k, d_v, n_heads, ff_dim)\n",
    "  attn_layer3 = TransformerEncoder(d_k, d_v, n_heads, ff_dim)\n",
    "\n",
    "  '''Construct model'''\n",
    "  in_seq = Input(shape=(seq_len, 6))\n",
    "  x = time_embedding(in_seq)\n",
    "  x = Concatenate(axis=-1)([in_seq, x])\n",
    "  x = attn_layer1((x, x, x))\n",
    "  x = attn_layer2((x, x, x))\n",
    "  x = attn_layer3((x, x, x))\n",
    "  x = GlobalAveragePooling1D(data_format='channels_first')(x)\n",
    "  x = Dropout(0.1)(x)\n",
    "  x = Dense(64, activation='relu')(x)\n",
    "  x = Dropout(0.1)(x)\n",
    "  out = Dense(1, activation='linear')(x)\n",
    "\n",
    "  model = Model(inputs=in_seq, outputs=out)\n",
    "  model.compile(loss='mse', optimizer='adam', metrics=['mae', 'mape'])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(real, predict):\n",
    "    real = np.array(real) + 1\n",
    "    predict = np.array(predict) + 1\n",
    "    percentage = 1 - np.sqrt(np.mean(np.square((real - predict) / real)))\n",
    "    return percentage * 100\n",
    "\n",
    "def anchor(signal, weight):\n",
    "    buffer = []\n",
    "    last = signal[0]\n",
    "    for i in signal:\n",
    "        smoothed_val = last * weight + (1 - weight) * i\n",
    "        buffer.append(smoothed_val)\n",
    "        last = smoothed_val\n",
    "    return buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forecast():\n",
    "    tf.compat.v1.reset_default_graph()\n",
    "    modelnn = Model(\n",
    "        learning_rate \n",
    "    )\n",
    "    sess = tf.InteractiveSession()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    date_ori = pd.to_datetime(df.iloc[:, 0]).tolist()\n",
    "\n",
    "    pbar = tqdm(range(epoch), desc = 'train loop')\n",
    "    for i in pbar:\n",
    "        total_loss, total_acc = [], []\n",
    "        for k in range(0, df_train.shape[0] - 1, timestamp):\n",
    "            index = min(k + timestamp, df_train.shape[0] - 1)\n",
    "            batch_x = np.expand_dims(\n",
    "                df_train.iloc[k : index, :].values, axis = 0\n",
    "            )\n",
    "            batch_y = df_train.iloc[k + 1 : index + 1, :].values\n",
    "            logits, _, loss = sess.run(\n",
    "                [modelnn.logits, modelnn.optimizer, modelnn.cost],\n",
    "                feed_dict = {modelnn.X: batch_x, modelnn.Y: batch_y},\n",
    "            )        \n",
    "            total_loss.append(loss)\n",
    "            total_acc.append(calculate_accuracy(batch_y[:, 0], logits[:, 0]))\n",
    "        pbar.set_postfix(cost = np.mean(total_loss), acc = np.mean(total_acc))\n",
    "    \n",
    "    future_day = test_size\n",
    "\n",
    "    output_predict = np.zeros((df_train.shape[0] + future_day, df_train.shape[1]))\n",
    "    output_predict[0] = df_train.iloc[0]\n",
    "    upper_b = (df_train.shape[0] // timestamp) * timestamp\n",
    "\n",
    "    for k in range(0, (df_train.shape[0] // timestamp) * timestamp, timestamp):\n",
    "        out_logits = sess.run(\n",
    "            modelnn.logits,\n",
    "            feed_dict = {\n",
    "                modelnn.X: np.expand_dims(\n",
    "                    df_train.iloc[k : k + timestamp], axis = 0\n",
    "                )\n",
    "            },\n",
    "        )\n",
    "        output_predict[k + 1 : k + timestamp + 1] = out_logits\n",
    "\n",
    "    if upper_b != df_train.shape[0]:\n",
    "        out_logits = sess.run(\n",
    "            modelnn.logits,\n",
    "            feed_dict = {\n",
    "                modelnn.X: np.expand_dims(df_train.iloc[upper_b:], axis = 0)\n",
    "            },\n",
    "        )\n",
    "        output_predict[upper_b + 1 : df_train.shape[0] + 1] = out_logits\n",
    "        future_day -= 1\n",
    "        date_ori.append(date_ori[-1] + timedelta(days = 1))\n",
    "    \n",
    "    for i in range(future_day):\n",
    "        o = output_predict[-future_day - timestamp + i:-future_day + i]\n",
    "        out_logits = sess.run(\n",
    "            modelnn.logits,\n",
    "            feed_dict = {\n",
    "                modelnn.X: np.expand_dims(o, axis = 0)\n",
    "            },\n",
    "        )\n",
    "        output_predict[-future_day + i] = out_logits[-1]\n",
    "        date_ori.append(date_ori[-1] + timedelta(days = 1))\n",
    "\n",
    "    output_predict = minmax.inverse_transform(output_predict)\n",
    "    deep_future = anchor(output_predict[:, 0], 0.3)\n",
    "    \n",
    "    return deep_future[-test_size:]"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6a4d3a8c6870627d9f3d27096c46470e773f4775cd8c21e672f38c44fbacec5e"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('oscar': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
